{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.datasets import CIFAR10\nfrom  torch.utils.data import DataLoader, Dataset\nfrom torch import nn, optim\nimport torchvision.transforms as transforms\nimport numpy as np\nimport pandas as pd\nimport copy\nfrom torchvision.transforms import Normalize\nfrom sklearn.model_selection import train_test_split\nfrom torch  import is_tensor\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torchvision.models as models\nfrom torchvision.models import mobilenet_v2, MobileNet_V2_Weights\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:55:46.784246Z","iopub.execute_input":"2024-04-26T15:55:46.784509Z","iopub.status.idle":"2024-04-26T15:55:54.423746Z","shell.execute_reply.started":"2024-04-26T15:55:46.784484Z","shell.execute_reply":"2024-04-26T15:55:54.422785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ahmedtarek1325/CNN-tutorial.git","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:55:54.425385Z","iopub.execute_input":"2024-04-26T15:55:54.425905Z","iopub.status.idle":"2024-04-26T15:55:55.948833Z","shell.execute_reply.started":"2024-04-26T15:55:54.425872Z","shell.execute_reply":"2024-04-26T15:55:55.947628Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'CNN-tutorial'...\nremote: Enumerating objects: 43, done.\u001b[K\nremote: Counting objects: 100% (43/43), done.\u001b[K\nremote: Compressing objects: 100% (32/32), done.\u001b[K\nremote: Total 43 (delta 9), reused 38 (delta 7), pack-reused 0\u001b[K\nUnpacking objects: 100% (43/43), 7.13 KiB | 730.00 KiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/CNN-tutorial')\nfrom tools import EarlyStopping, tune_model\nfrom data import train_test_split_ , perPixel_mean_std, perChannel_mean_std, build_transforms","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:55:58.424162Z","iopub.execute_input":"2024-04-26T15:55:58.425061Z","iopub.status.idle":"2024-04-26T15:55:58.436581Z","shell.execute_reply.started":"2024-04-26T15:55:58.425024Z","shell.execute_reply":"2024-04-26T15:55:58.435659Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Some Functions that will be used below**","metadata":{}},{"cell_type":"code","source":"def train_epoch2(model,dataloader,loss,optimizer,device):\n    model.train()\n    acc = []\n    lss_history = []\n    for _ , (data,labels) in enumerate(dataloader):\n        data = data.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        pred = model(data)\n        lss = loss(pred,labels)\n\n        lss.backward()\n        optimizer.step()\n\n\n        # acc calculations\n        lss_history.append(lss.item())\n        acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\n\n\n    return np.mean(lss_history) ,np.mean(acc)\n\ndef validate_epoch2(model,dataloader,loss,device):\n    model.eval()\n    acc = []\n    lss_history = []\n    with torch.no_grad():\n        for i , (data,labels) in enumerate(dataloader):\n            data = data.to(device)\n            labels = labels.to(device)\n\n            pred = model(data)\n            lss = loss(pred,labels)\n            lss_history.append(lss.item())\n            acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\n    return np.mean(lss_history) ,np.mean(acc)\n\ndef tune_model2(num_epochs,model,train_dataloader_,test_dataloader_,\\\n               loss,optimizer,device,scheduler=None,earlystopping=None) :\n    '''\n    NOTE that the scheduler here takes the update after every epoch not step\n    '''\n\n    hist = {'train_loss': [],\n            'train_acc':[],\n            'test_loss': [],\n            'test_acc':[]}\n\n    last_lr = optimizer.state_dict()['param_groups'][0]['lr']\n    f= 0\n    for e in range(num_epochs):\n        lss,acc= train_epoch2(model,train_dataloader_,loss,optimizer,device)\n        test_lss,test_acc= validate_epoch2(model,test_dataloader_,loss,device)\n\n\n        print(f\"For epoch {e:3d} || Training Loss {lss:5.3f} || acc {acc:5.3f}\",end='')\n        print(f\" || Testing Loss {test_lss:5.3f} || Test acc {test_acc:5.3f}\")\n        hist['train_loss'].append(lss)\n        hist['train_acc'].append(acc)\n        hist['test_loss'].append(test_lss)\n        hist['test_acc'].append(test_acc)\n\n        #\n        if earlystopping:\n            if earlystopping(model,test_lss): # should terminate\n                print('Early Stopping Activated')\n                return hist\n        # if you have scheduler\n        if scheduler:\n            scheduler.step(test_lss)\n            try:\n        # applying manual verbose for the scheduler\n                if last_lr != scheduler.get_last_lr()[0]:\n                    print(f'scheduler update at Epoch {e+1}')\n                    last_lr = scheduler.get_last_lr()[0]\n            except:\n                f+=1\n    return hist\n\nclass MyDataset2(Dataset):\n    def __init__(self,x,y,transforms = None):\n        self.x = x\n        self.y = y\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self,idx):\n        img = Image.fromarray(self.x[idx])\n        if self.transforms is not None:\n            x= self.transforms(img)\n            return x.type(torch.float32),self.y[idx]\n\n        return self.x[idx],self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:01.447698Z","iopub.execute_input":"2024-04-26T15:56:01.448405Z","iopub.status.idle":"2024-04-26T15:56:01.467473Z","shell.execute_reply.started":"2024-04-26T15:56:01.448372Z","shell.execute_reply":"2024-04-26T15:56:01.466532Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Defining CNNBlock**","metadata":{}},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, cin, cout, stride=1, groups=1):\n        super().__init__()\n        self.downsample = False\n        self.groups = groups\n        self.cnn = nn.Conv2d(cin,cout,3,padding=1,stride=stride,bias=False)\n\n        if groups==1: # same implementation of normal ResNet\n            self.cnn2 = nn.Conv2d(cout,cout,3,padding=1,bias=False)\n            if stride !=1:\n                self.projection  = nn.Conv2d(cin,cout,1,stride=stride,bias=False)\n                self.downsample = True\n\n        else: # implementation for ResNext (grouped convolutions) --> My colleague Lydia helped me in this part\n            # Reduce dimension\n            self.cnn1 = nn.Conv2d(cout, cout // 2, 1, bias=False)\n            # Grouped convolutions\n            self.cnn2 = nn.Conv2d(cout // 2, cout // 2, 3, padding=1, groups=groups, bias=False)\n            # Increase dimension\n            self.cnn3 = nn.Conv2d(cout // 2, cout, 1, bias=False)\n\n            if stride != 1:\n                self.projection = nn.Conv2d(cin, cout, 1, stride=stride, bias=False, groups=groups)\n                self.downsample = True\n\n        self.act1 = nn.ReLU()\n        self.act2 = nn.ReLU()\n        self.BN1 = nn.BatchNorm2d(cout)\n        self.BN2 = nn.BatchNorm2d(cout)\n    \n    def forward(self, x):\n        out1 = self.act1(self.BN1(self.cnn(x)))\n\n        if self.groups == 1:\n            out2 = self.act2(self.BN2(self.cnn2(out1)))\n            if self.downsample:\n                return out2 + self.projection(x)\n            return out2 + x\n        else:\n            grouping = self.cnn3(self.cnn2(self.cnn1(out1))) \n            if self.downsample:\n                return self.act2(self.BN2(grouping))\n            return self.act2(self.BN2(grouping) + x)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:06.453718Z","iopub.execute_input":"2024-04-26T15:56:06.454738Z","iopub.status.idle":"2024-04-26T15:56:06.467537Z","shell.execute_reply.started":"2024-04-26T15:56:06.454703Z","shell.execute_reply":"2024-04-26T15:56:06.466512Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Defining ResNext Architecture**","metadata":{}},{"cell_type":"code","source":"class ResNext(nn.Module):\n    def __init__(self, layersPerStage, filtersPerStage, groups=1):\n        super().__init__()\n        f1, f2, f3 = filtersPerStage\n        self.baseCNN = nn.Conv2d(3, f1, 3, padding=1)\n        self.base = nn.Sequential(nn.BatchNorm2d(f1), nn.ReLU())\n\n        self.stage1 = nn.Sequential(*[CNNBlock(f1, f1, groups=groups) for _ in range(layersPerStage[0])])\n        \n        layers = [CNNBlock(f1, f2, 2, groups)]\n        layers.extend([CNNBlock(f2, f2, groups=groups) for _ in range(layersPerStage[1] - 1)])\n        self.stage2 = nn.Sequential(*layers)\n\n        layers = [CNNBlock(f2, f3, 2, groups)]\n        layers.extend([CNNBlock(f3, f3, groups=groups) for _ in range(layersPerStage[2] - 1)])\n        self.stage3 = nn.Sequential(*layers)\n\n        self.head = nn.Sequential(\n            nn.AvgPool2d(2,2),\n            nn.Flatten(),\n            nn.Linear(f3*4*4, 10)\n        )\n\n    def forward(self, x):\n        o1 = self.base(self.baseCNN(x))\n        o2 = self.stage1(o1)\n        o3 = self.stage2(o2)\n        o4 = self.stage3(o3)\n        return self.head(o4)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:14.610071Z","iopub.execute_input":"2024-04-26T15:56:14.610921Z","iopub.status.idle":"2024-04-26T15:56:14.620948Z","shell.execute_reply.started":"2024-04-26T15:56:14.610887Z","shell.execute_reply":"2024-04-26T15:56:14.619951Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Model Definitions**","metadata":{}},{"cell_type":"code","source":"model1 = ResNext([5]*3, [16, 32, 64], groups=1)\n\nmodel2 = ResNext([5]*3, [16, 32, 64], groups=4)\n\nmodel3 = ResNext([5]*3, [32, 64, 128], groups=4)\n\n# A fourth model of my choice. More grouping\nmodel4 = ResNext([6, 7, 8], [32, 64, 128], groups=8)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:18.212824Z","iopub.execute_input":"2024-04-26T15:56:18.213190Z","iopub.status.idle":"2024-04-26T15:56:18.343126Z","shell.execute_reply.started":"2024-04-26T15:56:18.213158Z","shell.execute_reply":"2024-04-26T15:56:18.342350Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self,x,y,transforms = None):\n        self.x = x\n        self.y = y\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self,idx):\n        if self.transforms is not None:\n            x= self.transforms(image= self.x[idx])['image']\n            return x.type(torch.float32),self.y[idx]\n\n        return self.x[idx],self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:21.076749Z","iopub.execute_input":"2024-04-26T15:56:21.077434Z","iopub.status.idle":"2024-04-26T15:56:21.084406Z","shell.execute_reply.started":"2024-04-26T15:56:21.077398Z","shell.execute_reply":"2024-04-26T15:56:21.083430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data  = CIFAR10(root='./data',train = True,download = True)\ntest_data = CIFAR10(root='/data',train=False,download = True)\n\n# make a validation set\nconfig= {'random_state' : 42,\n          'shuffle':True,\n         'train_size':0.9 }\nX_train, X_val, y_train, y_val= train_test_split_(train_data.data,train_data.targets,config)\nX_train.shape,X_val.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:23.628289Z","iopub.execute_input":"2024-04-26T15:56:23.629109Z","iopub.status.idle":"2024-04-26T15:56:44.337952Z","shell.execute_reply.started":"2024-04-26T15:56:23.629070Z","shell.execute_reply":"2024-04-26T15:56:44.337011Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:06<00:00, 28334219.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:08<00:00, 20162215.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /data/cifar-10-python.tar.gz to /data\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((45000, 32, 32, 3), (5000, 32, 32, 3))"},"metadata":{}}]},{"cell_type":"code","source":"# stratify to keep the ratio 0.9 for every class in the trainined and testing\nuni, counts = np.unique(y_val, return_counts=True)\ncounts","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:44.339839Z","iopub.execute_input":"2024-04-26T15:56:44.340234Z","iopub.status.idle":"2024-04-26T15:56:44.347710Z","shell.execute_reply.started":"2024-04-26T15:56:44.340180Z","shell.execute_reply":"2024-04-26T15:56:44.346773Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([500, 500, 500, 500, 500, 500, 500, 500, 500, 500])"},"metadata":{}}]},{"cell_type":"code","source":"mean_,std_ = perChannel_mean_std(X_train,'NHWC')\nmean_,std_","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:46.348561Z","iopub.execute_input":"2024-04-26T15:56:46.349429Z","iopub.status.idle":"2024-04-26T15:56:47.190965Z","shell.execute_reply.started":"2024-04-26T15:56:46.349396Z","shell.execute_reply":"2024-04-26T15:56:47.190052Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(array([125.24002457, 122.93835269, 113.84449184]),\n array([62.95162973, 62.07468216, 66.70886545]))"},"metadata":{}}]},{"cell_type":"code","source":"train_tansforms,test_transforms = build_transforms('pipeline1',{'mean':mean_,\n                                                                 'std':std_})","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:48.524842Z","iopub.execute_input":"2024-04-26T15:56:48.525234Z","iopub.status.idle":"2024-04-26T15:56:48.530999Z","shell.execute_reply.started":"2024-04-26T15:56:48.525182Z","shell.execute_reply":"2024-04-26T15:56:48.529906Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainset = MyDataset(X_train,y_train,train_tansforms)\ntrainloader_ = DataLoader(trainset,batch_size=128, shuffle=True)\n\nvalset = MyDataset(X_val,y_val,test_transforms)\nvalloader_ = DataLoader(trainset,batch_size=1024, shuffle=False)\n\ntestset = MyDataset(test_data.data,test_data.targets,test_transforms)\ntestloader_ = DataLoader(testset,batch_size=512, shuffle=False)\n\ndevice= 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:50.472580Z","iopub.execute_input":"2024-04-26T15:56:50.473041Z","iopub.status.idle":"2024-04-26T15:56:50.535777Z","shell.execute_reply.started":"2024-04-26T15:56:50.473002Z","shell.execute_reply":"2024-04-26T15:56:50.534654Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"loss = nn.CrossEntropyLoss()\nearlystopping = EarlyStopping()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:53.285617Z","iopub.execute_input":"2024-04-26T15:56:53.286000Z","iopub.status.idle":"2024-04-26T15:56:53.290495Z","shell.execute_reply.started":"2024-04-26T15:56:53.285968Z","shell.execute_reply":"2024-04-26T15:56:53.289446Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **Model 1**","metadata":{}},{"cell_type":"code","source":"model1.to(device)\noptimizer = optim.Adam(model1.parameters(),lr= 0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience = 5,factor = 0.1,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:08.440755Z","iopub.execute_input":"2024-04-26T02:01:08.441502Z","iopub.status.idle":"2024-04-26T02:01:08.454873Z","shell.execute_reply.started":"2024-04-26T02:01:08.441470Z","shell.execute_reply":"2024-04-26T02:01:08.454078Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"hist_= tune_model(100,model1,trainloader_,valloader_,\\\n               loss,optimizer,device,scheduler,earlystopping=earlystopping)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:29.337812Z","iopub.execute_input":"2024-04-26T02:01:29.338167Z","iopub.status.idle":"2024-04-26T02:42:57.732678Z","shell.execute_reply.started":"2024-04-26T02:01:29.338142Z","shell.execute_reply":"2024-04-26T02:42:57.731671Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"For epoch   4 || Training Loss 0.602 || acc 0.790 || Testing Loss 0.681 || Test acc 0.764\nFor epoch   9 || Training Loss 0.421 || acc 0.854 || Testing Loss 0.427 || Test acc 0.853\nFor epoch  14 || Training Loss 0.319 || acc 0.888 || Testing Loss 0.355 || Test acc 0.877\nFor epoch  19 || Training Loss 0.257 || acc 0.911 || Testing Loss 0.247 || Test acc 0.914\nFor epoch  24 || Training Loss 0.217 || acc 0.925 || Testing Loss 0.212 || Test acc 0.925\nFor epoch  29 || Training Loss 0.190 || acc 0.934 || Testing Loss 0.162 || Test acc 0.944\nFor epoch  34 || Training Loss 0.161 || acc 0.942 || Testing Loss 0.144 || Test acc 0.950\nFor epoch  39 || Training Loss 0.144 || acc 0.949 || Testing Loss 0.199 || Test acc 0.931\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"code","source":"print('best loss is',earlystopping.best_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:47:20.562749Z","iopub.execute_input":"2024-04-26T02:47:20.563177Z","iopub.status.idle":"2024-04-26T02:47:20.568869Z","shell.execute_reply.started":"2024-04-26T02:47:20.563146Z","shell.execute_reply":"2024-04-26T02:47:20.567791Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"best loss is 0.14418904618783432\n","output_type":"stream"}]},{"cell_type":"code","source":"acc=validate_epoch2(model1,testloader_,loss,device)[1] # though the function is called validate_epoch2, it aims to see the accuracy of whatever the passed dataloader\n# Therefore, since I am passing testloader rather than valloader, it will give me the testing accuracy\nprint(\"Model 1 accuracy: \", acc*100)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:47:38.213966Z","iopub.execute_input":"2024-04-26T02:47:38.214606Z","iopub.status.idle":"2024-04-26T02:47:39.941320Z","shell.execute_reply.started":"2024-04-26T02:47:38.214574Z","shell.execute_reply":"2024-04-26T02:47:39.940325Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Model 1 accuracy:  87.49195784330368\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Model 2**","metadata":{}},{"cell_type":"code","source":"model2.to(device)\noptimizer2 = optim.Adam(model2.parameters(),lr= 0.001)\nscheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2,'min',patience = 5,factor = 0.1,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T13:20:38.753826Z","iopub.execute_input":"2024-04-26T13:20:38.754545Z","iopub.status.idle":"2024-04-26T13:20:38.935480Z","shell.execute_reply.started":"2024-04-26T13:20:38.754514Z","shell.execute_reply":"2024-04-26T13:20:38.934746Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"hist_2= tune_model(100,model2,trainloader_,valloader_,\\\n               loss,optimizer2,device,scheduler2,earlystopping=earlystopping)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T13:20:42.981404Z","iopub.execute_input":"2024-04-26T13:20:42.982009Z","iopub.status.idle":"2024-04-26T14:15:58.510951Z","shell.execute_reply.started":"2024-04-26T13:20:42.981976Z","shell.execute_reply":"2024-04-26T14:15:58.510004Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"For epoch   4 || Training Loss 0.814 || acc 0.715 || Testing Loss 0.832 || Test acc 0.709\nFor epoch   9 || Training Loss 0.603 || acc 0.790 || Testing Loss 0.846 || Test acc 0.720\nFor epoch  14 || Training Loss 0.503 || acc 0.825 || Testing Loss 0.530 || Test acc 0.815\nFor epoch  19 || Training Loss 0.431 || acc 0.848 || Testing Loss 0.600 || Test acc 0.792\nFor epoch  24 || Training Loss 0.378 || acc 0.868 || Testing Loss 0.408 || Test acc 0.859\nFor epoch  29 || Training Loss 0.336 || acc 0.882 || Testing Loss 0.328 || Test acc 0.884\nFor epoch  34 || Training Loss 0.308 || acc 0.893 || Testing Loss 0.297 || Test acc 0.897\nFor epoch  39 || Training Loss 0.276 || acc 0.903 || Testing Loss 0.289 || Test acc 0.899\nFor epoch  44 || Training Loss 0.253 || acc 0.912 || Testing Loss 0.251 || Test acc 0.911\nFor epoch  49 || Training Loss 0.238 || acc 0.917 || Testing Loss 0.227 || Test acc 0.920\nFor epoch  54 || Training Loss 0.221 || acc 0.922 || Testing Loss 0.218 || Test acc 0.924\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"code","source":"print('best loss is',earlystopping.best_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T14:15:58.513054Z","iopub.execute_input":"2024-04-26T14:15:58.513771Z","iopub.status.idle":"2024-04-26T14:15:58.518267Z","shell.execute_reply.started":"2024-04-26T14:15:58.513733Z","shell.execute_reply":"2024-04-26T14:15:58.517405Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"best loss is 0.20463847843083469\n","output_type":"stream"}]},{"cell_type":"code","source":"acc2=validate_epoch2(model2,testloader_,loss,device)[1]\nprint('Model 2 accuracy: ', acc2*100)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T14:16:01.721088Z","iopub.execute_input":"2024-04-26T14:16:01.721373Z","iopub.status.idle":"2024-04-26T14:16:03.325409Z","shell.execute_reply.started":"2024-04-26T14:16:01.721346Z","shell.execute_reply":"2024-04-26T14:16:03.324409Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model 2 accuracy:  85.74850648641586\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Model 3**","metadata":{}},{"cell_type":"code","source":"model3.to(device)\noptimizer3 = optim.Adam(model3.parameters(),lr= 0.001)\nscheduler3 = optim.lr_scheduler.ReduceLROnPlateau(optimizer3,'min',patience = 5,factor = 0.1,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T14:37:05.739977Z","iopub.execute_input":"2024-04-26T14:37:05.740331Z","iopub.status.idle":"2024-04-26T14:37:05.753045Z","shell.execute_reply.started":"2024-04-26T14:37:05.740301Z","shell.execute_reply":"2024-04-26T14:37:05.752177Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"hist_3= tune_model(100,model3,trainloader_,valloader_,\\\n               loss,optimizer3,device,scheduler3,earlystopping=earlystopping)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T14:37:07.538583Z","iopub.execute_input":"2024-04-26T14:37:07.538952Z","iopub.status.idle":"2024-04-26T15:45:25.375088Z","shell.execute_reply.started":"2024-04-26T14:37:07.538925Z","shell.execute_reply":"2024-04-26T15:45:25.373909Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"For epoch   4 || Training Loss 0.663 || acc 0.769 || Testing Loss 0.673 || Test acc 0.766\nFor epoch   9 || Training Loss 0.473 || acc 0.836 || Testing Loss 0.497 || Test acc 0.828\nFor epoch  14 || Training Loss 0.372 || acc 0.871 || Testing Loss 0.413 || Test acc 0.858\nFor epoch  19 || Training Loss 0.300 || acc 0.896 || Testing Loss 0.297 || Test acc 0.896\nFor epoch  24 || Training Loss 0.251 || acc 0.913 || Testing Loss 0.281 || Test acc 0.900\nFor epoch  29 || Training Loss 0.216 || acc 0.924 || Testing Loss 0.201 || Test acc 0.929\nFor epoch  34 || Training Loss 0.179 || acc 0.937 || Testing Loss 0.183 || Test acc 0.936\nFor epoch  39 || Training Loss 0.156 || acc 0.945 || Testing Loss 0.165 || Test acc 0.942\nFor epoch  44 || Training Loss 0.136 || acc 0.953 || Testing Loss 0.182 || Test acc 0.937\nFor epoch  49 || Training Loss 0.119 || acc 0.958 || Testing Loss 0.110 || Test acc 0.962\nFor epoch  54 || Training Loss 0.113 || acc 0.960 || Testing Loss 0.129 || Test acc 0.955\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"code","source":"print('best loss is',earlystopping.best_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:48:02.168577Z","iopub.execute_input":"2024-04-26T15:48:02.168980Z","iopub.status.idle":"2024-04-26T15:48:02.174203Z","shell.execute_reply.started":"2024-04-26T15:48:02.168948Z","shell.execute_reply":"2024-04-26T15:48:02.173227Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"best loss is 0.11035843921655958\n","output_type":"stream"}]},{"cell_type":"code","source":"acc3=validate_epoch2(model3,testloader_,loss,device)[1]\nprint('Model 3 accuracy: ', acc3*100)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:48:04.430056Z","iopub.execute_input":"2024-04-26T15:48:04.430416Z","iopub.status.idle":"2024-04-26T15:48:07.114204Z","shell.execute_reply.started":"2024-04-26T15:48:04.430386Z","shell.execute_reply":"2024-04-26T15:48:07.113231Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Model 3 accuracy:  88.58053773641586\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Model 4**","metadata":{}},{"cell_type":"code","source":"model4.to(device)\noptimizer4 = optim.Adam(model4.parameters(),lr= 0.001)\nscheduler4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer4,'min',patience = 5,factor = 0.1,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:57:12.973099Z","iopub.execute_input":"2024-04-26T15:57:12.973959Z","iopub.status.idle":"2024-04-26T15:57:13.154196Z","shell.execute_reply.started":"2024-04-26T15:57:12.973925Z","shell.execute_reply":"2024-04-26T15:57:13.153254Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"hist_4= tune_model(100,model4,trainloader_,valloader_,\\\n               loss,optimizer4,device,scheduler4,earlystopping=earlystopping)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:57:15.446839Z","iopub.execute_input":"2024-04-26T15:57:15.447689Z","iopub.status.idle":"2024-04-26T17:32:11.696295Z","shell.execute_reply.started":"2024-04-26T15:57:15.447654Z","shell.execute_reply":"2024-04-26T17:32:11.695256Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"For epoch   4 || Training Loss 0.762 || acc 0.737 || Testing Loss 0.872 || Test acc 0.718\nFor epoch   9 || Training Loss 0.502 || acc 0.827 || Testing Loss 0.490 || Test acc 0.835\nFor epoch  14 || Training Loss 0.381 || acc 0.866 || Testing Loss 0.378 || Test acc 0.870\nFor epoch  19 || Training Loss 0.308 || acc 0.894 || Testing Loss 0.327 || Test acc 0.888\nFor epoch  24 || Training Loss 0.250 || acc 0.912 || Testing Loss 0.261 || Test acc 0.907\nFor epoch  29 || Training Loss 0.208 || acc 0.927 || Testing Loss 0.198 || Test acc 0.931\nFor epoch  34 || Training Loss 0.172 || acc 0.940 || Testing Loss 0.157 || Test acc 0.945\nFor epoch  39 || Training Loss 0.148 || acc 0.948 || Testing Loss 0.133 || Test acc 0.954\nFor epoch  44 || Training Loss 0.131 || acc 0.954 || Testing Loss 0.121 || Test acc 0.958\nFor epoch  49 || Training Loss 0.111 || acc 0.962 || Testing Loss 0.121 || Test acc 0.958\nFor epoch  54 || Training Loss 0.100 || acc 0.965 || Testing Loss 0.098 || Test acc 0.965\nFor epoch  59 || Training Loss 0.091 || acc 0.967 || Testing Loss 0.086 || Test acc 0.970\nFor epoch  64 || Training Loss 0.083 || acc 0.971 || Testing Loss 0.087 || Test acc 0.969\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"code","source":"print('best loss is',earlystopping.best_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:36:58.755709Z","iopub.execute_input":"2024-04-26T17:36:58.756648Z","iopub.status.idle":"2024-04-26T17:36:58.761343Z","shell.execute_reply.started":"2024-04-26T17:36:58.756613Z","shell.execute_reply":"2024-04-26T17:36:58.760397Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"best loss is 0.07060314570976929\n","output_type":"stream"}]},{"cell_type":"code","source":"acc4=validate_epoch2(model4,testloader_,loss,device)[1]\nprint('Model 4 accuracy: ', acc4*100)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:37:01.071140Z","iopub.execute_input":"2024-04-26T17:37:01.071990Z","iopub.status.idle":"2024-04-26T17:37:03.918183Z","shell.execute_reply.started":"2024-04-26T17:37:01.071957Z","shell.execute_reply":"2024-04-26T17:37:03.917271Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model 4 accuracy:  88.83444398641586\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Part 3: Pre-trained architectures (MobileNetV2)**","metadata":{}},{"cell_type":"markdown","source":"**Freezing all layers except the classification layer**","metadata":{}},{"cell_type":"code","source":"frozen_model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n\n# Freeze all the parameters in the model\nfor parameter in frozen_model.parameters():\n    parameter.requires_grad = False # this ensures that I am not updating anything\n\n# Preparing the classifier layer to be updated\nfrozen_model.classifier[1] = nn.Linear(frozen_model.classifier[1].in_features, 10)\n\n# updating only the parameters of the classifier\nfrozen_optimizer = optim.Adam(frozen_model.classifier.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:59:42.624729Z","iopub.execute_input":"2024-04-25T21:59:42.625417Z","iopub.status.idle":"2024-04-25T21:59:42.744110Z","shell.execute_reply.started":"2024-04-25T21:59:42.625385Z","shell.execute_reply":"2024-04-25T21:59:42.743023Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## **Pre-processing the images before using MobileNetV2. According to this link: https://pytorch.org/hub/pytorch_vision_mobilenet_v2/**","metadata":{}},{"cell_type":"code","source":"preprocess_train = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\npreprocess_val_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrainset_mobilenet = MyDataset2(X_train, y_train, transforms=preprocess_train)\nvalset_mobileenet = MyDataset2(X_val, y_val, transforms=preprocess_val_test)\ntestset_mobilenet = MyDataset2(test_data.data, test_data.targets, transforms=preprocess_val_test)\n\ntrainloader_mobilenet = DataLoader(trainset_mobilenet, batch_size=128, shuffle=True)\nvalloader_mobilenet = DataLoader(valset_mobileenet, batch_size=1024, shuffle=False)\ntestloader_mobilenet = DataLoader(testset_mobilenet, batch_size=512, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:56:08.770495Z","iopub.execute_input":"2024-04-25T22:56:08.771395Z","iopub.status.idle":"2024-04-25T22:56:08.780319Z","shell.execute_reply.started":"2024-04-25T22:56:08.771361Z","shell.execute_reply":"2024-04-25T22:56:08.779283Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"frozen_model.to(device)\nscheduler_frozen = optim.lr_scheduler.ReduceLROnPlateau(frozen_optimizer,'min',patience = 5,factor = 0.1,verbose=True)\nloss = nn.CrossEntropyLoss()\nearlystopping = EarlyStopping()\nhist_frozen= tune_model2(100,frozen_model,trainloader_mobilenet,testloader_mobilenet,\\\n               loss,frozen_optimizer,device,scheduler_frozen,earlystopping=earlystopping)\n\n# Notice I am here passing testloader and not valloader. That means I am directly testing on the test dataset and not on a validation dataset.","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:09:48.174689Z","iopub.execute_input":"2024-04-25T23:09:48.175110Z","iopub.status.idle":"2024-04-26T00:48:47.140080Z","shell.execute_reply.started":"2024-04-25T23:09:48.175079Z","shell.execute_reply":"2024-04-26T00:48:47.138945Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"For epoch   0 || Training Loss 0.886 || acc 0.699 || Testing Loss 0.795 || Test acc 0.737\nFor epoch   1 || Training Loss 0.862 || acc 0.706 || Testing Loss 0.781 || Test acc 0.743\nFor epoch   2 || Training Loss 0.849 || acc 0.710 || Testing Loss 0.768 || Test acc 0.744\nFor epoch   3 || Training Loss 0.845 || acc 0.711 || Testing Loss 0.764 || Test acc 0.742\nFor epoch   4 || Training Loss 0.835 || acc 0.714 || Testing Loss 0.750 || Test acc 0.749\nFor epoch   5 || Training Loss 0.826 || acc 0.715 || Testing Loss 0.748 || Test acc 0.747\nFor epoch   6 || Training Loss 0.820 || acc 0.719 || Testing Loss 0.739 || Test acc 0.750\nFor epoch   7 || Training Loss 0.816 || acc 0.717 || Testing Loss 0.736 || Test acc 0.749\nFor epoch   8 || Training Loss 0.805 || acc 0.720 || Testing Loss 0.730 || Test acc 0.752\nFor epoch   9 || Training Loss 0.803 || acc 0.722 || Testing Loss 0.722 || Test acc 0.756\nFor epoch  10 || Training Loss 0.801 || acc 0.724 || Testing Loss 0.724 || Test acc 0.755\nFor epoch  11 || Training Loss 0.801 || acc 0.721 || Testing Loss 0.719 || Test acc 0.755\nFor epoch  12 || Training Loss 0.793 || acc 0.723 || Testing Loss 0.715 || Test acc 0.755\nFor epoch  13 || Training Loss 0.794 || acc 0.723 || Testing Loss 0.709 || Test acc 0.758\nFor epoch  14 || Training Loss 0.795 || acc 0.725 || Testing Loss 0.710 || Test acc 0.758\nFor epoch  15 || Training Loss 0.789 || acc 0.726 || Testing Loss 0.702 || Test acc 0.762\nFor epoch  16 || Training Loss 0.791 || acc 0.728 || Testing Loss 0.701 || Test acc 0.759\nFor epoch  17 || Training Loss 0.784 || acc 0.727 || Testing Loss 0.706 || Test acc 0.757\nFor epoch  18 || Training Loss 0.785 || acc 0.729 || Testing Loss 0.699 || Test acc 0.760\nFor epoch  19 || Training Loss 0.782 || acc 0.726 || Testing Loss 0.699 || Test acc 0.759\nFor epoch  20 || Training Loss 0.788 || acc 0.726 || Testing Loss 0.696 || Test acc 0.762\nFor epoch  21 || Training Loss 0.779 || acc 0.729 || Testing Loss 0.695 || Test acc 0.761\nFor epoch  22 || Training Loss 0.779 || acc 0.728 || Testing Loss 0.693 || Test acc 0.763\nFor epoch  23 || Training Loss 0.781 || acc 0.727 || Testing Loss 0.696 || Test acc 0.761\nFor epoch  24 || Training Loss 0.770 || acc 0.734 || Testing Loss 0.690 || Test acc 0.761\nFor epoch  25 || Training Loss 0.769 || acc 0.734 || Testing Loss 0.689 || Test acc 0.762\nFor epoch  26 || Training Loss 0.775 || acc 0.730 || Testing Loss 0.685 || Test acc 0.765\nFor epoch  27 || Training Loss 0.771 || acc 0.731 || Testing Loss 0.685 || Test acc 0.765\nFor epoch  28 || Training Loss 0.773 || acc 0.730 || Testing Loss 0.684 || Test acc 0.765\nFor epoch  29 || Training Loss 0.769 || acc 0.730 || Testing Loss 0.685 || Test acc 0.762\nFor epoch  30 || Training Loss 0.772 || acc 0.732 || Testing Loss 0.681 || Test acc 0.763\nFor epoch  31 || Training Loss 0.765 || acc 0.732 || Testing Loss 0.681 || Test acc 0.766\nFor epoch  32 || Training Loss 0.769 || acc 0.730 || Testing Loss 0.679 || Test acc 0.766\nFor epoch  33 || Training Loss 0.767 || acc 0.733 || Testing Loss 0.680 || Test acc 0.766\nFor epoch  34 || Training Loss 0.770 || acc 0.732 || Testing Loss 0.683 || Test acc 0.764\nFor epoch  35 || Training Loss 0.769 || acc 0.731 || Testing Loss 0.681 || Test acc 0.765\nFor epoch  36 || Training Loss 0.768 || acc 0.732 || Testing Loss 0.681 || Test acc 0.764\nFor epoch  37 || Training Loss 0.769 || acc 0.729 || Testing Loss 0.680 || Test acc 0.765\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training all layers**","metadata":{}},{"cell_type":"code","source":"full_model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n\nfor param in full_model.parameters():\n    param.requires_grad = True\n\nfull_model.classifier[1] = nn.Linear(full_model.classifier[1].in_features, 10)\n\nfull_optimizer = optim.Adam(full_model.parameters(), lr=0.001) # notice here that I did not write \".classifier\", which means I am updating everything","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:52:00.301108Z","iopub.execute_input":"2024-04-26T00:52:00.302061Z","iopub.status.idle":"2024-04-26T00:52:00.419483Z","shell.execute_reply.started":"2024-04-26T00:52:00.302025Z","shell.execute_reply":"2024-04-26T00:52:00.418616Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"full_model.to(device)\nscheduler_full = optim.lr_scheduler.ReduceLROnPlateau(full_optimizer,'min',patience = 5,factor = 0.1,verbose=True)\nloss = nn.CrossEntropyLoss()\nearlystopping = EarlyStopping()\nhist_full= tune_model2(100,full_model,trainloader_mobilenet,testloader_mobilenet,\\\n               loss,full_optimizer,device,scheduler_full,earlystopping=earlystopping)\n\n# Notice I am here passing testloader and not valloader. That means I am directly testing on the test dataset and not on a validation dataset.","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:52:18.269719Z","iopub.execute_input":"2024-04-26T00:52:18.270638Z","iopub.status.idle":"2024-04-26T01:54:51.159164Z","shell.execute_reply.started":"2024-04-26T00:52:18.270603Z","shell.execute_reply":"2024-04-26T01:54:51.158130Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"For epoch   0 || Training Loss 0.507 || acc 0.826 || Testing Loss 0.415 || Test acc 0.861\nFor epoch   1 || Training Loss 0.324 || acc 0.889 || Testing Loss 0.331 || Test acc 0.889\nFor epoch   2 || Training Loss 0.266 || acc 0.910 || Testing Loss 0.332 || Test acc 0.889\nFor epoch   3 || Training Loss 0.229 || acc 0.921 || Testing Loss 0.331 || Test acc 0.892\nFor epoch   4 || Training Loss 0.215 || acc 0.927 || Testing Loss 0.296 || Test acc 0.906\nFor epoch   5 || Training Loss 0.191 || acc 0.935 || Testing Loss 0.257 || Test acc 0.914\nFor epoch   6 || Training Loss 0.179 || acc 0.939 || Testing Loss 0.279 || Test acc 0.910\nFor epoch   7 || Training Loss 0.160 || acc 0.944 || Testing Loss 0.264 || Test acc 0.916\nFor epoch   8 || Training Loss 0.153 || acc 0.947 || Testing Loss 0.243 || Test acc 0.923\nFor epoch   9 || Training Loss 0.143 || acc 0.950 || Testing Loss 0.261 || Test acc 0.917\nFor epoch  10 || Training Loss 0.137 || acc 0.952 || Testing Loss 0.251 || Test acc 0.921\nFor epoch  11 || Training Loss 0.129 || acc 0.954 || Testing Loss 0.256 || Test acc 0.918\nFor epoch  12 || Training Loss 0.122 || acc 0.958 || Testing Loss 0.276 || Test acc 0.919\nFor epoch  13 || Training Loss 0.118 || acc 0.959 || Testing Loss 0.244 || Test acc 0.924\nEarly Stopping Activated\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Discussion**","metadata":{}},{"cell_type":"markdown","source":"The fully-trained model is definitely much better. This is because MobileNet is originally trained on ImageNet data, not CIFAR-10. So, I think that exposing it to training on the new data improves its accuracy when compared to only training the classification layer. I didn't experiment much with augmentation, changing the optimizer, etc. due to time, but I think the conclusion would remain the same.","metadata":{}},{"cell_type":"markdown","source":"# **Brief essay on the model**","metadata":{}},{"cell_type":"markdown","source":"MobileNetV2 is a major breakthrough in the world of deep learning. Building on the original MobileNet's success, MobileNetV2 offered a number of obvious enhancements. The usage of inverted residual blocks with linear bottleneck layers, which permits improved representation learning while preserving computational efficiency, is one noteworthy breakthrough.Specifically, MobileNetV2 makes use of a unique linear bottleneck topology that promotes more efficient information flow across the network. The introduction of a linear bottleneck layer between inverted residuals, which improves gradient flow during training, is an important addition that other models haven't tackled. As the name suggests, it is a very \"mobile\" model, which means it is light and very computationally efficient.","metadata":{}}]}