{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"data:image/png;base64,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\"\n",
        " style=\"float:right;width:50px;height:50px;\">\n",
        "\n",
        "<h1 align=\"center\">Zewail University of Science and Technology\n",
        "</h1>\n",
        "\n",
        "<h1 align=\"center\">Biomedical Sciences_ Computational Biology and Genomics_BMS 474</h1>\n",
        "<h1 align=\"center\">FALL 2023\t</h1>\n",
        "<h1 align=\"center\">Assignment 3 </h1>\n"
      ],
      "metadata": {
        "id": "E2ZdmzVyhHp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Code Setup: Decision Tree"
      ],
      "metadata": {
        "id": "ezU2zFYWhcON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "Ez4Xbby7dP0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL-iAjsUfKSg"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avUzvyYufP1g"
      },
      "source": [
        "# 1. Train-Test-Split\n",
        "def train_test_split(df, test_size, seed=None): # I edited this function\n",
        "\n",
        "    if isinstance(test_size, float):\n",
        "        test_size = round(test_size * len(df))\n",
        "\n",
        "    if seed is not None:\n",
        "      random.seed(seed)\n",
        "\n",
        "    indices = df.index.tolist()\n",
        "    test_indices = random.sample(population=indices, k=test_size)\n",
        "\n",
        "    test_df = df.loc[test_indices]\n",
        "    train_df = df.drop(test_indices)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "# 2. Distinguish categorical and continuous features\n",
        "def determine_type_of_feature(df):\n",
        "\n",
        "    feature_types = []\n",
        "    n_unique_values_treshold = 15\n",
        "    for feature in df.columns:\n",
        "        if feature != \"label\":\n",
        "            unique_values = df[feature].unique()\n",
        "            example_value = unique_values[0]\n",
        "\n",
        "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
        "                feature_types.append(\"categorical\")\n",
        "            else:\n",
        "                feature_types.append(\"continuous\")\n",
        "\n",
        "    return feature_types\n",
        "\n",
        "\n",
        "# 3. Accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    predictions_correct = predictions == labels\n",
        "    accuracy = predictions_correct.mean()\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh25u4Ynf0OY"
      },
      "source": [
        "##Desision Tree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Data pure?\n",
        "def check_purity(data):\n",
        "\n",
        "    label_column = data[:, -1]\n",
        "    unique_classes = np.unique(label_column)\n",
        "\n",
        "    if len(unique_classes) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "# 1.2 Classify\n",
        "def classify_data(data):\n",
        "\n",
        "    label_column = data[:, -1]\n",
        "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
        "\n",
        "    index = counts_unique_classes.argmax()\n",
        "    classification = unique_classes[index]\n",
        "\n",
        "    return classification\n",
        "\n",
        "\n",
        "# 1.3 Potential splits?\n",
        "def get_potential_splits(data, random_subspace):\n",
        "\n",
        "    potential_splits = {}\n",
        "    _, n_columns = data.shape\n",
        "    column_indices = list(range(n_columns - 1))    # excluding the last column which is the label\n",
        "\n",
        "    if random_subspace and random_subspace <= len(column_indices):\n",
        "        column_indices = random.sample(population=column_indices, k=random_subspace)\n",
        "\n",
        "    for column_index in column_indices:\n",
        "        values = data[:, column_index]\n",
        "        unique_values = np.unique(values)\n",
        "\n",
        "        potential_splits[column_index] = unique_values\n",
        "\n",
        "    return potential_splits\n",
        "\n",
        "\n",
        "# 1.4 Lowest Overall Entropy?\n",
        "def calculate_entropy(data):\n",
        "\n",
        "    label_column = data[:, -1]\n",
        "    _, counts = np.unique(label_column, return_counts=True)\n",
        "\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def calculate_overall_entropy(data_below, data_above):\n",
        "\n",
        "    n = len(data_below) + len(data_above)\n",
        "    p_data_below = len(data_below) / n\n",
        "    p_data_above = len(data_above) / n\n",
        "\n",
        "    overall_entropy =  (p_data_below * calculate_entropy(data_below)\n",
        "                      + p_data_above * calculate_entropy(data_above))\n",
        "\n",
        "    return overall_entropy\n",
        "\n",
        "\n",
        "def determine_best_split(data, potential_splits):\n",
        "\n",
        "    overall_entropy = 9999\n",
        "    for column_index in potential_splits:\n",
        "        for value in potential_splits[column_index]:\n",
        "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
        "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
        "\n",
        "            if current_overall_entropy <= overall_entropy:\n",
        "                overall_entropy = current_overall_entropy\n",
        "                best_split_column = column_index\n",
        "                best_split_value = value\n",
        "\n",
        "    return best_split_column, best_split_value\n",
        "\n",
        "\n",
        "# 1.5 Split data\n",
        "def split_data(data, split_column, split_value):\n",
        "\n",
        "    split_column_values = data[:, split_column]\n",
        "\n",
        "    type_of_feature = FEATURE_TYPES[split_column]\n",
        "    if type_of_feature == \"continuous\":\n",
        "        data_below = data[split_column_values <= split_value]\n",
        "        data_above = data[split_column_values >  split_value]\n",
        "\n",
        "    # feature is categorical\n",
        "    else:\n",
        "        data_below = data[split_column_values == split_value]\n",
        "        data_above = data[split_column_values != split_value]\n",
        "\n",
        "    return data_below, data_above\n",
        "\n",
        "\n",
        "# 2. Decision Tree Algorithm\n",
        "def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5, random_subspace=None):\n",
        "\n",
        "    # data preparations\n",
        "    if counter == 0:\n",
        "        global COLUMN_HEADERS, FEATURE_TYPES\n",
        "        COLUMN_HEADERS = df.columns\n",
        "        FEATURE_TYPES = determine_type_of_feature(df)\n",
        "        data = df.values\n",
        "    else:\n",
        "        data = df\n",
        "\n",
        "\n",
        "    # base cases\n",
        "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
        "        classification = classify_data(data)\n",
        "\n",
        "        return classification\n",
        "\n",
        "\n",
        "    # recursive part\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "        # helper functions\n",
        "        potential_splits = get_potential_splits(data, random_subspace)\n",
        "        split_column, split_value = determine_best_split(data, potential_splits)\n",
        "        data_below, data_above = split_data(data, split_column, split_value)\n",
        "\n",
        "        # check for empty data\n",
        "        if len(data_below) == 0 or len(data_above) == 0:\n",
        "            classification = classify_data(data)\n",
        "            return classification\n",
        "\n",
        "        # determine question\n",
        "        feature_name = COLUMN_HEADERS[split_column]\n",
        "        type_of_feature = FEATURE_TYPES[split_column]\n",
        "        if type_of_feature == \"continuous\":\n",
        "            question = \"{} <= {}\".format(feature_name, split_value)\n",
        "\n",
        "        # feature is categorical\n",
        "        else:\n",
        "            question = \"{} = {}\".format(feature_name, split_value)\n",
        "\n",
        "        # instantiate sub-tree\n",
        "        sub_tree = {question: []}\n",
        "\n",
        "        # find answers (recursion)\n",
        "        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth, random_subspace)\n",
        "        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth, random_subspace)\n",
        "\n",
        "        # If the answers are the same, then there is no point in asking the qestion.\n",
        "        # This could happen when the data is classified even though it is not pure\n",
        "        # yet (min_samples or max_depth base case).\n",
        "        if yes_answer == no_answer:\n",
        "            sub_tree = yes_answer\n",
        "        else:\n",
        "            sub_tree[question].append(yes_answer)\n",
        "            sub_tree[question].append(no_answer)\n",
        "\n",
        "        return sub_tree\n",
        "\n",
        "\n",
        "# 3. Make predictions\n",
        "# 3.1 One example\n",
        "def predict_example(example, tree):\n",
        "    question = list(tree.keys())[0]\n",
        "    feature_name, comparison_operator, value = question.rsplit(\" \", 2)\n",
        "\n",
        "    # ask question\n",
        "    if comparison_operator == \"<=\":\n",
        "        if example[feature_name] <= float(value):\n",
        "            answer = tree[question][0]\n",
        "        else:\n",
        "            answer = tree[question][1]\n",
        "\n",
        "    # feature is categorical\n",
        "    else:\n",
        "        if str(example[feature_name]) == value:\n",
        "            answer = tree[question][0]\n",
        "        else:\n",
        "            answer = tree[question][1]\n",
        "\n",
        "    # base case\n",
        "    if not isinstance(answer, dict):\n",
        "        return answer\n",
        "\n",
        "    # recursive part\n",
        "    else:\n",
        "        residual_tree = answer\n",
        "        return predict_example(example, residual_tree)\n",
        "\n",
        "\n",
        "# 3.2 All examples of the test data\n",
        "def decision_tree_predictions(test_df, tree):\n",
        "    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "BjFozmLU2dO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Build your own Random Forest classifier from scratch\n",
        "\n",
        "By following the steps below, you will gain a deeper understanding of how Random Forest works and how it combines multiple Decision Trees for improved performance.\n",
        "\n",
        "Implement the following functions for Random Forest using the above Decision Tree implementation."
      ],
      "metadata": {
        "id": "-uw74oERdNMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1: Resampling(train_df, n_bootstrap)\n",
        "      \n",
        "- It performs Resampling on a given training dataset train_df.\n",
        "\n",
        "- Resampling is a statistical technique that involves random sampling with replacement.\n",
        "\n",
        "- It performs random sampling with replacement to create a new dataset containing a subset of the original rows.\n",
        "      \n",
        "- This technique is commonly used in ensemble learning methods like Random Forest to create diverse subsets of data for training individual models."
      ],
      "metadata": {
        "id": "ab1nha3xkf-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a function that takes the dataframe, the number of bootstraps, and an optional seed. It implements resampling and returns the bootstrapped dataframe\n",
        "def resampling(train_df, n_bootstrap, seed=None):\n",
        "  if seed is not None:\n",
        "    random.seed(seed)\n",
        "  resampling_indices = np.random.randint(low=0, high=len(train_df), size=n_bootstrap) # generate n_boostrap number of indices\n",
        "  bootstrap_df = train_df.iloc[resampling_indices] # use the indices to subset from the original dataframe\n",
        "\n",
        "  return bootstrap_df"
      ],
      "metadata": {
        "id": "6AORiIqsG-Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2: random_forest_Combined(train_df, n_trees, n_bootstrap, n_features, dt_max_depth)\n",
        "\n",
        "  - It implements the **random forest algorithm** from scratch.\n",
        "  \n",
        "  - It builds a random forest by repeatedly performing bootstrapping and constructing decision trees.\n",
        "      \n",
        "  - Each decision tree is built using a bootstrapped sample and a subset of randomly selected features.\n",
        "      \n",
        "  - The resulting random forest is a collection of decision trees that can be used for prediction or classification tasks."
      ],
      "metadata": {
        "id": "Rx3Bumdvk6OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a function that takes as inputs the parameters of the decision tree as well as the resampling function to do a RandomForest\n",
        "# This function should build n_trees number of trees, where each tree is of a certain bootstrap\n",
        "# All the trees should be appened to a \"RandomForest\" array\n",
        "def random_forest_combined(train_df, n_trees, n_bootstrap, n_features, dt_max_depth, seed):\n",
        "  RandomForest = []\n",
        "  for i in range(n_trees):\n",
        "    # Bootstrap the data\n",
        "        bootstrap_df = resampling(train_df, n_bootstrap, seed) # using the function I built above\n",
        "        # Build a decision tree on the bootstrapped data\n",
        "        decision_tree = decision_tree_algorithm(bootstrap_df, max_depth = dt_max_depth, random_subspace = n_features) # if we do not specify values for the last 4 parameters, it is okay because they have default values...\n",
        "        RandomForest.append(decision_tree)\n",
        "\n",
        "  return RandomForest # the output is a list of n_trees decision trees, each with a certain bootstrapping (subset of the original data)\n"
      ],
      "metadata": {
        "id": "9RU1tz41HsRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.3: random_forest_predictions(test_df, forest)\n",
        "  - It is used to make predictions using a trained random forest on a given test dataset test_df\n",
        "  \n",
        "  - It iterates over each decision tree in the forest, makes predictions using the corresponding tree,and stores the predictions in a DataFrame.\n",
        "  \n",
        "  - The final predictions for the random forest are determined by taking the mode of the predictions across all decision trees."
      ],
      "metadata": {
        "id": "6-48d5JolCzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# This is a function that takes a test dataframe as an input as well as the RandomForest model that contains n_trees number of trees\n",
        "def random_forest_predictions(test_df, forest):\n",
        "  predictions_df = pd.DataFrame() # creating a dataframe to save the predictions of every tree\n",
        "\n",
        "  # then I loop over all the RandomForest trees, checking the accuracy of every decision tree\n",
        "  for i in range(len(forest)):\n",
        "    predictions = decision_tree_predictions(test_df.iloc[:,:-1], forest[i]) # making predictions using the already implemented function decision_tree_predictions\n",
        "    predictions_df['tree_' + str(i)] = predictions\n",
        "  # predictions_df is now 2D dataframe where the columns are called 'tree_0', 'tree_1', until 'tree_n_trees - 1'\n",
        "  # and the rows are the samples in the test_df\n",
        "  # Every sample in the test_df has n predictions across all the trees. For every sample, we want the mode.. so we want the mode across the columns\n",
        "\n",
        "  random_forest_predictions = stats.mode(predictions_df, axis=1)[0] # The mode of all predictions using stats function\n",
        "  # The [0] because stats.mode returns an object where the first element is the mode itself, and the second element is its count\n",
        "  # I only want the mode, not its count, so I index the first element\n",
        "\n",
        "  # random_forest_predictions is now expected to be a 2D array where every rows corresponds to the mode prediction score. It is expetced to be 2D because this is how stats.mode works. It takes a 2D dataframe and outputs a 2D array.\n",
        "  # I only want the mode scores themselves, so I flatten it to become 1D. The flattened array should have the mode prediction scores, where every element corresponds to a certain sample\n",
        "\n",
        "  return random_forest_predictions.flatten() # this is a 1-D array that will be suitable for the calculate_accuracy function"
      ],
      "metadata": {
        "id": "DLUF1UnQKWNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2:\n",
        "Use your RF classifier you just built from scratch in classifying iris-data.Then, compare its accuracy to using decision tree."
      ],
      "metadata": {
        "id": "JCTFljIeru_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1: Use your RF classifier you just built from scratch in classifying iris-data.\n",
        "\n",
        "Using\n",
        "seed =0,\n",
        "n_trees=4,\n",
        "n_bootstrap=800,\n",
        "n_features=4,\n",
        "dt_max_depth=4\n"
      ],
      "metadata": {
        "id": "3xpLzAl9jA29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "n_trees = 4\n",
        "n_bootstrap = 800\n",
        "n_features = 4\n",
        "dt_max_depth = 4\n",
        "\n",
        "iris_df = load_iris() # Loading the iris dataframe\n",
        "\n",
        "X, Y = iris_df.data, iris_df.target # Separating the features and the labels\n",
        "\n",
        "iris_df = pd.DataFrame(X, columns=iris_df.feature_names) # Converting the scikit-learn array into a dataframe, where the data is the features and the column names are the feature names from the feature_names attribute of the load_iris function\n",
        "\n",
        "iris_df['label'] = Y # Adding a column called label where its values are the values in the Y array\n",
        "\n",
        "train_df, test_df = train_test_split(iris_df, test_size=0.2, seed=0) # Splitting the data into training and testing using the helper function from above\n",
        "# I will put seed = 0 here only, but not in the bootstrapping function\n",
        "\n",
        "forest = random_forest_combined(train_df, n_trees=n_trees, n_bootstrap=n_bootstrap, n_features=n_features, dt_max_depth=dt_max_depth) # Random Forest Classifier\n",
        "# The optimal way to choose the parameters would be to do as we did in Lab 7 to find the optimal combination :'D\n",
        "rf_predictions = random_forest_predictions(test_df, forest) # calling the random_forest_predictions function I just made above\n",
        "# Note I pass test_df and not test_df.iloc[:,:-1] because random_forest_predictions handles this inside it..\n",
        "# So if I actually pass test_df.iloc[:,:-1] this means that two columns will be removed, the label column and the one preciding it\n",
        "rf_accuracy = calculate_accuracy(rf_predictions, test_df['label'].values) # Calling the helper function calculate_accuracy, which takes the predicted labels and the true labels as inputs to calculate the accuracy (mean score)\n",
        "\n",
        "# After running the code with everything as it is, it was giving me the error below, which means that rf_predictions = random_forest_predictions(test_df, forest) was producing an error\n",
        "# because it was calling the predict_example function\n",
        "# After analyzing the error, it seems a problem with the splitting of the predict_example function.\n",
        "# Eventually rsplit(\" \", 2) helped resolving it. Why?\n",
        "# I depended on this link to arrive at this solution: https://www.w3schools.com/python/ref_string_rsplit.asp#:~:text=The%20rsplit()%20method%20splits,number%20of%20elements%20plus%20one.\n",
        "\n",
        "# 167 def predict_example(example, tree):\n",
        "#     168     question = list(tree.keys())[0]\n",
        "# --> 169     feature_name, comparison_operator, value = question.split(\" \")\n",
        "#     170\n",
        "#     171     # ask question\n",
        "\n",
        "# ValueError: too many values to unpack (expected 3)"
      ],
      "metadata": {
        "id": "JwgD9NBVSHhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2: lets compare it to using DT"
      ],
      "metadata": {
        "id": "neDMZTVti75R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the same but for the decision tree classifier\n",
        "tree = decision_tree_algorithm(train_df, max_depth=dt_max_depth, random_subspace = n_features) # I guess the random_subspace is equivalent to the number of features? I don't remember :'D\n",
        "dt_predictions = decision_tree_predictions(test_df.iloc[:, :-1], tree)\n",
        "dt_accuracy = calculate_accuracy(dt_predictions.values, test_df['label'].values)\n",
        "\n",
        "# Comparing the accuracies\n",
        "print(\"Random Forest Accuracy: \", rf_accuracy) # 0.9333333333333333 - for a given seed of 0 in train_test_split\n",
        "print(\"Decision Tree Accuracy: \", dt_accuracy) # 0.9333333333333333 - for a given seed of 0 in train_test_split\n",
        "\n",
        "# Their accuracies range from 0.8666666666666667 to 1.0, they're very similar tho. Maybe RF could be enhanced if the parameters are optimized. Also, this is an educational dataset, so it's very neat and clean."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLD6FbSDcsC3",
        "outputId": "8ceb18dd-24ee-4aaa-8a80-4527e8d53875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy:  0.9333333333333333\n",
            "Decision Tree Accuracy:  0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3: Use the GSE45827 dataset for primary invasive breast cancer\n",
        "\n",
        "you are  required to do the following using Scikit package :\n",
        "\n",
        "1- Apply normalization step using standard scalar class (2 mark)\n",
        "\n",
        "2- Perform PCA on the features (FV length= 50). (2 mark)\n",
        "\n",
        "3- Use the  Random Forest classifier in the classification step (No. of decision trees is 20). (2 mark)\n",
        "\n",
        "4- Split your data into training and testing (%80 training set-%20 testing) and report your accuracy (F score). (3 marks)\n",
        "\n",
        "5- Now, implement K fold cross validation for evaluation (K = 5) and report the accuracy (F-score). (5 marks)\n",
        "\n",
        "6- Discuss the results you got from 4 and 5. (2 mark)\n",
        "\n",
        "7- Try different number of trees[50,60,70,80,90,100] in the random forest algorithm and decide which one is better(2 mark)\n"
      ],
      "metadata": {
        "id": "X69APz7nc1v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All the neccessary imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import GEOparse\n",
        "\n",
        "# 1. First, I will load the GSE45827 dataset using GEOparse package\n",
        "gse = GEOparse.get_GEO(geo=\"GSE45827\")\n",
        "\n",
        "# 2. Then, I will do all the neccessary pre-processing\n",
        "\n",
        "# This dataset contains 130 tumor samples as well as 11 normal tissues samples and 14 cell lines, for a total of 155 samples\n",
        "\n",
        "dataframes = {} # a dictionary to save the sample name and its gene expression dataframe\n",
        "for gsm_name, gsm in gse.gsms.items():\n",
        "    df = gsm.table.copy() # the gene expression dataframe\n",
        "    dataframes[gsm_name] = df # key = sample name and value = gene expression dataframe\n",
        "\n",
        "# The below block of code converts a dictionary with key as sample name and value as dataframe to one dataframe where the column is the sample name and rows are the genes\n",
        "df_list = [] # a list to save all dataframes, where each dataframe consists of 1 column (sample name) and 29873 rows (gene names), and the cells themselves are the expression values of the genes\n",
        "for sample_name, df in dataframes.items(): # I loop over the dictionary I made\n",
        "  df = df.set_index('ID_REF') # I change the gene expression dataframe (which is the value of the dictionary) such that its indices are the ID_REFs, which is the column used for gene name\n",
        "  # This means that for every gene expression dataframe, it has 29873 rows which are the 29873 gene names\n",
        "  df.columns = [sample_name] # Next, I manipulate the columns of the dataframe such that I only retain the sample_name\n",
        "\n",
        "  # After all of these steps, I have a gene expression dataframe of 29873 gene names and 1 column, which is the sample name\n",
        "\n",
        "  df_list.append(df) # I append my manipulated gene expression dataframe to my df_list\n",
        "\n",
        "# df_list contains 155 dataframes, which corresponds for the 155 samples\n",
        "# I want to merge them all in one big dataframe, so I use concat function across axis=1\n",
        "final_df = pd.concat(df_list, axis=1)  # Now this is the final dataframe, which has as rows genes and as columns samples\n",
        "\n",
        "final_df = final_df.T # I will transpose it to align with PCA later (such that features are columns and not rows)\n",
        "final_df = final_df.rename_axis('Sample_Acc', axis='columns') # I change the axis (column) name such that it's Sample_Acc instead of ID_REF because I transposed, so the first column is for the sample names rather than gene names\n",
        "\n",
        "# now, the dataframe needs an additional columns for the label. Therefore, I will make a dictionary where the key is the sample_acc and the value is the subtype\n",
        "\n",
        "subtype_dict = {}\n",
        "for gsm_name, gsm in gse.gsms.items():\n",
        "    subtype = gsm.metadata['title'][0]  # Extract subtype\n",
        "    Samples_index = subtype.find('Sample') - 1 # I need to retain the letters till the word \"Sample\", so I find its index\n",
        "    subtype_dict[gsm_name] = subtype[:Samples_index] # I slice and add the subtype as a value for the sample accession key\n",
        "\n",
        "# Now, subtype_dict contains the sample accession and the subtype..\n",
        "# I want to use this information to add an additional for every row (every sample accession) with the corresponding label\n",
        "\n",
        "# This code maps the accessions in the dataframe (with a certain order) to their subtypes from the subtype_dict\n",
        "# The output is a list with subtypes with the exact order as the accessions of the dataframe\n",
        "subtypes = []\n",
        "for accession in final_df.index:\n",
        "  subtypes.append(subtype_dict[accession])\n",
        "\n",
        "final_df['Subtype'] = subtypes # I can simply now add a new column for the subtypes\n",
        "\n",
        "# Now, FINALLY... I have a dataframe with 155 rows and 29874 columns (genes + label)!!\n",
        "\n",
        "final_df.isna().sum().sum() # there are no nulls in expression values, so I don't remove anything...\n",
        "\n",
        "final_df.columns.isna().sum() # no nulls in column names\n",
        "\n",
        "final_df['Subtype'].isna().sum() # no nulls in labels\n",
        "\n",
        "# Now, I want to remove any gene with expression of 0 in more than 25% of the samples\n",
        "\n",
        "# Calculate the percentage of zero expression per gene (column)\n",
        "zero_expression_percent = (final_df == 0).mean() * 100 # this is inspired by the following rationale\n",
        "# final_df == 0 generates an array of booleans (TRUE, TRUE, FALSE, and so on)\n",
        "# whenever a gene has zero expression, it is true, and whenever it is not 0, it is false\n",
        "# True means 1, and false means 0\n",
        "# therefore, if we have an array [TRUE, TRUE, FALSE] for example, it means that 66% of the data has zero expression\n",
        "# This is calculated as follows: 1(TRUE, expression = 0) + 1(TRUE, expression=0) + 0(FALSE, expression != 0) / 3 = 0.66, which I multiply by 100 to be 66.66%\n",
        "# Therefore, it is general that by getting the mean of true and falses, which are 1's and 0's, I can get the proportion of the samples that do not express the genes\n",
        "\n",
        "# Now, I shall use this to keep only the genes with 0 expression in less than or equal to 25% of the samples\n",
        "\n",
        "genes_to_keep = zero_expression_percent[zero_expression_percent <= 25].index\n",
        "final_df = final_df[genes_to_keep]\n",
        "\n",
        "# Apparently, nothing is removed, but it's okay :D"
      ],
      "metadata": {
        "id": "_A4D__LCeSQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "wgpAj_9euiWS",
        "outputId": "03402ff1-8a64-4921-c079-ad9a2fcc3879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sample_Acc  1007_s_at  1053_at   117_at  1294_at  1316_at  1405_i_at  1438_at  \\\n",
              "GSM1116084    9.47065  8.36311  5.95426  6.02119  3.22997   10.82220  4.39698   \n",
              "GSM1116085    9.67440  8.72194  7.02523  7.24581  3.29352    9.29455  6.68936   \n",
              "GSM1116086   10.20800  7.78601  6.39671  6.85310  3.26204    9.45727  5.46440   \n",
              "GSM1116087   10.11420  9.44537  4.56023  5.42786  3.34766   11.51270  5.29748   \n",
              "GSM1116088   11.16360  7.71242  5.29008  7.51120  3.59086    8.83075  6.81821   \n",
              "...               ...      ...      ...      ...      ...        ...      ...   \n",
              "GSM1116234   10.55470  6.88277  4.14597  6.37882  3.55309    6.36287  3.58838   \n",
              "GSM1116235    9.45290  6.83700  6.10766  6.99674  3.23899    9.56499  5.11154   \n",
              "GSM1116236   10.80010  7.29371  5.41169  8.37598  3.84879    8.11443  7.23567   \n",
              "GSM1116237   10.04640  7.71122  4.57742  6.56774  3.28091    8.07087  3.43906   \n",
              "GSM1116238   11.42010  7.63877  4.67159  7.40374  3.24882    8.44304  3.38312   \n",
              "\n",
              "Sample_Acc  1487_at  1552256_a_at  1552257_a_at  ...  89476_r_at  90265_at  \\\n",
              "GSM1116084  6.65320      10.91320      10.07510  ...     7.67662   8.58529   \n",
              "GSM1116085  7.20820       9.32204       9.34476  ...     7.32080   6.21915   \n",
              "GSM1116086  6.68974       7.73131       8.43573  ...     7.53787   7.86806   \n",
              "GSM1116087  6.87276      10.39210       9.63893  ...     7.86515   8.79839   \n",
              "GSM1116088  6.45744       7.05836       9.37463  ...     7.33174   7.01692   \n",
              "...             ...           ...           ...  ...         ...       ...   \n",
              "GSM1116234  6.44435       8.15810       6.76867  ...     7.36935   7.40855   \n",
              "GSM1116235  5.82317       7.34009       8.50502  ...     7.59543   8.17949   \n",
              "GSM1116236  6.02314       9.16813       8.68150  ...     7.56045   8.15041   \n",
              "GSM1116237  8.44026       7.81914       8.75455  ...     7.32272   6.97906   \n",
              "GSM1116238  7.35658      10.20410       7.84748  ...     8.19394   7.02052   \n",
              "\n",
              "Sample_Acc  90610_at  91617_at  91684_g_at  91703_at  91816_f_at  91826_at  \\\n",
              "GSM1116084   6.58532   6.47111     8.05685   5.51480     6.04568   5.90542   \n",
              "GSM1116085   6.81551   6.54900     6.62661   5.58320     5.67661   6.68275   \n",
              "GSM1116086   6.90606   6.32751     7.51148   5.85887     3.14257   7.77158   \n",
              "GSM1116087   7.09756   6.03216     7.38772   4.55774     3.04537   6.47312   \n",
              "GSM1116088   5.51992   6.09710     6.41229   5.34572     4.33680   8.48681   \n",
              "...              ...       ...         ...       ...         ...       ...   \n",
              "GSM1116234   6.24758   5.05863     6.07391   4.92928     2.82099   7.16552   \n",
              "GSM1116235   7.14055   5.46896     6.57521   5.46086     3.74439   5.70158   \n",
              "GSM1116236   7.09543   6.06971     8.32561   4.97832     2.93524   7.88820   \n",
              "GSM1116237   7.16655   6.46210     6.53545   4.98447     2.94121   7.27446   \n",
              "GSM1116238   7.44111   7.15645     6.52246   4.85768     3.03094   7.39253   \n",
              "\n",
              "Sample_Acc  91952_at    Subtype  \n",
              "GSM1116084   4.34839      Basal  \n",
              "GSM1116085   4.94622      Basal  \n",
              "GSM1116086   3.58973       Her2  \n",
              "GSM1116087   4.55167      Basal  \n",
              "GSM1116088   4.72397       Her2  \n",
              "...              ...        ...  \n",
              "GSM1116234   3.85854  Luminal A  \n",
              "GSM1116235   4.25533  Luminal A  \n",
              "GSM1116236   3.93143  Luminal B  \n",
              "GSM1116237   3.70314  Luminal B  \n",
              "GSM1116238   4.41307  Luminal B  \n",
              "\n",
              "[155 rows x 29874 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7668f13-2b94-4055-b3ff-b53582ebefa0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Sample_Acc</th>\n",
              "      <th>1007_s_at</th>\n",
              "      <th>1053_at</th>\n",
              "      <th>117_at</th>\n",
              "      <th>1294_at</th>\n",
              "      <th>1316_at</th>\n",
              "      <th>1405_i_at</th>\n",
              "      <th>1438_at</th>\n",
              "      <th>1487_at</th>\n",
              "      <th>1552256_a_at</th>\n",
              "      <th>1552257_a_at</th>\n",
              "      <th>...</th>\n",
              "      <th>89476_r_at</th>\n",
              "      <th>90265_at</th>\n",
              "      <th>90610_at</th>\n",
              "      <th>91617_at</th>\n",
              "      <th>91684_g_at</th>\n",
              "      <th>91703_at</th>\n",
              "      <th>91816_f_at</th>\n",
              "      <th>91826_at</th>\n",
              "      <th>91952_at</th>\n",
              "      <th>Subtype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>GSM1116084</th>\n",
              "      <td>9.47065</td>\n",
              "      <td>8.36311</td>\n",
              "      <td>5.95426</td>\n",
              "      <td>6.02119</td>\n",
              "      <td>3.22997</td>\n",
              "      <td>10.82220</td>\n",
              "      <td>4.39698</td>\n",
              "      <td>6.65320</td>\n",
              "      <td>10.91320</td>\n",
              "      <td>10.07510</td>\n",
              "      <td>...</td>\n",
              "      <td>7.67662</td>\n",
              "      <td>8.58529</td>\n",
              "      <td>6.58532</td>\n",
              "      <td>6.47111</td>\n",
              "      <td>8.05685</td>\n",
              "      <td>5.51480</td>\n",
              "      <td>6.04568</td>\n",
              "      <td>5.90542</td>\n",
              "      <td>4.34839</td>\n",
              "      <td>Basal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116085</th>\n",
              "      <td>9.67440</td>\n",
              "      <td>8.72194</td>\n",
              "      <td>7.02523</td>\n",
              "      <td>7.24581</td>\n",
              "      <td>3.29352</td>\n",
              "      <td>9.29455</td>\n",
              "      <td>6.68936</td>\n",
              "      <td>7.20820</td>\n",
              "      <td>9.32204</td>\n",
              "      <td>9.34476</td>\n",
              "      <td>...</td>\n",
              "      <td>7.32080</td>\n",
              "      <td>6.21915</td>\n",
              "      <td>6.81551</td>\n",
              "      <td>6.54900</td>\n",
              "      <td>6.62661</td>\n",
              "      <td>5.58320</td>\n",
              "      <td>5.67661</td>\n",
              "      <td>6.68275</td>\n",
              "      <td>4.94622</td>\n",
              "      <td>Basal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116086</th>\n",
              "      <td>10.20800</td>\n",
              "      <td>7.78601</td>\n",
              "      <td>6.39671</td>\n",
              "      <td>6.85310</td>\n",
              "      <td>3.26204</td>\n",
              "      <td>9.45727</td>\n",
              "      <td>5.46440</td>\n",
              "      <td>6.68974</td>\n",
              "      <td>7.73131</td>\n",
              "      <td>8.43573</td>\n",
              "      <td>...</td>\n",
              "      <td>7.53787</td>\n",
              "      <td>7.86806</td>\n",
              "      <td>6.90606</td>\n",
              "      <td>6.32751</td>\n",
              "      <td>7.51148</td>\n",
              "      <td>5.85887</td>\n",
              "      <td>3.14257</td>\n",
              "      <td>7.77158</td>\n",
              "      <td>3.58973</td>\n",
              "      <td>Her2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116087</th>\n",
              "      <td>10.11420</td>\n",
              "      <td>9.44537</td>\n",
              "      <td>4.56023</td>\n",
              "      <td>5.42786</td>\n",
              "      <td>3.34766</td>\n",
              "      <td>11.51270</td>\n",
              "      <td>5.29748</td>\n",
              "      <td>6.87276</td>\n",
              "      <td>10.39210</td>\n",
              "      <td>9.63893</td>\n",
              "      <td>...</td>\n",
              "      <td>7.86515</td>\n",
              "      <td>8.79839</td>\n",
              "      <td>7.09756</td>\n",
              "      <td>6.03216</td>\n",
              "      <td>7.38772</td>\n",
              "      <td>4.55774</td>\n",
              "      <td>3.04537</td>\n",
              "      <td>6.47312</td>\n",
              "      <td>4.55167</td>\n",
              "      <td>Basal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116088</th>\n",
              "      <td>11.16360</td>\n",
              "      <td>7.71242</td>\n",
              "      <td>5.29008</td>\n",
              "      <td>7.51120</td>\n",
              "      <td>3.59086</td>\n",
              "      <td>8.83075</td>\n",
              "      <td>6.81821</td>\n",
              "      <td>6.45744</td>\n",
              "      <td>7.05836</td>\n",
              "      <td>9.37463</td>\n",
              "      <td>...</td>\n",
              "      <td>7.33174</td>\n",
              "      <td>7.01692</td>\n",
              "      <td>5.51992</td>\n",
              "      <td>6.09710</td>\n",
              "      <td>6.41229</td>\n",
              "      <td>5.34572</td>\n",
              "      <td>4.33680</td>\n",
              "      <td>8.48681</td>\n",
              "      <td>4.72397</td>\n",
              "      <td>Her2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116234</th>\n",
              "      <td>10.55470</td>\n",
              "      <td>6.88277</td>\n",
              "      <td>4.14597</td>\n",
              "      <td>6.37882</td>\n",
              "      <td>3.55309</td>\n",
              "      <td>6.36287</td>\n",
              "      <td>3.58838</td>\n",
              "      <td>6.44435</td>\n",
              "      <td>8.15810</td>\n",
              "      <td>6.76867</td>\n",
              "      <td>...</td>\n",
              "      <td>7.36935</td>\n",
              "      <td>7.40855</td>\n",
              "      <td>6.24758</td>\n",
              "      <td>5.05863</td>\n",
              "      <td>6.07391</td>\n",
              "      <td>4.92928</td>\n",
              "      <td>2.82099</td>\n",
              "      <td>7.16552</td>\n",
              "      <td>3.85854</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116235</th>\n",
              "      <td>9.45290</td>\n",
              "      <td>6.83700</td>\n",
              "      <td>6.10766</td>\n",
              "      <td>6.99674</td>\n",
              "      <td>3.23899</td>\n",
              "      <td>9.56499</td>\n",
              "      <td>5.11154</td>\n",
              "      <td>5.82317</td>\n",
              "      <td>7.34009</td>\n",
              "      <td>8.50502</td>\n",
              "      <td>...</td>\n",
              "      <td>7.59543</td>\n",
              "      <td>8.17949</td>\n",
              "      <td>7.14055</td>\n",
              "      <td>5.46896</td>\n",
              "      <td>6.57521</td>\n",
              "      <td>5.46086</td>\n",
              "      <td>3.74439</td>\n",
              "      <td>5.70158</td>\n",
              "      <td>4.25533</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116236</th>\n",
              "      <td>10.80010</td>\n",
              "      <td>7.29371</td>\n",
              "      <td>5.41169</td>\n",
              "      <td>8.37598</td>\n",
              "      <td>3.84879</td>\n",
              "      <td>8.11443</td>\n",
              "      <td>7.23567</td>\n",
              "      <td>6.02314</td>\n",
              "      <td>9.16813</td>\n",
              "      <td>8.68150</td>\n",
              "      <td>...</td>\n",
              "      <td>7.56045</td>\n",
              "      <td>8.15041</td>\n",
              "      <td>7.09543</td>\n",
              "      <td>6.06971</td>\n",
              "      <td>8.32561</td>\n",
              "      <td>4.97832</td>\n",
              "      <td>2.93524</td>\n",
              "      <td>7.88820</td>\n",
              "      <td>3.93143</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116237</th>\n",
              "      <td>10.04640</td>\n",
              "      <td>7.71122</td>\n",
              "      <td>4.57742</td>\n",
              "      <td>6.56774</td>\n",
              "      <td>3.28091</td>\n",
              "      <td>8.07087</td>\n",
              "      <td>3.43906</td>\n",
              "      <td>8.44026</td>\n",
              "      <td>7.81914</td>\n",
              "      <td>8.75455</td>\n",
              "      <td>...</td>\n",
              "      <td>7.32272</td>\n",
              "      <td>6.97906</td>\n",
              "      <td>7.16655</td>\n",
              "      <td>6.46210</td>\n",
              "      <td>6.53545</td>\n",
              "      <td>4.98447</td>\n",
              "      <td>2.94121</td>\n",
              "      <td>7.27446</td>\n",
              "      <td>3.70314</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GSM1116238</th>\n",
              "      <td>11.42010</td>\n",
              "      <td>7.63877</td>\n",
              "      <td>4.67159</td>\n",
              "      <td>7.40374</td>\n",
              "      <td>3.24882</td>\n",
              "      <td>8.44304</td>\n",
              "      <td>3.38312</td>\n",
              "      <td>7.35658</td>\n",
              "      <td>10.20410</td>\n",
              "      <td>7.84748</td>\n",
              "      <td>...</td>\n",
              "      <td>8.19394</td>\n",
              "      <td>7.02052</td>\n",
              "      <td>7.44111</td>\n",
              "      <td>7.15645</td>\n",
              "      <td>6.52246</td>\n",
              "      <td>4.85768</td>\n",
              "      <td>3.03094</td>\n",
              "      <td>7.39253</td>\n",
              "      <td>4.41307</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>155 rows × 29874 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7668f13-2b94-4055-b3ff-b53582ebefa0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7668f13-2b94-4055-b3ff-b53582ebefa0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7668f13-2b94-4055-b3ff-b53582ebefa0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec4c416b-f061-4960-b9f2-8c0f5c35cf26\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec4c416b-f061-4960-b9f2-8c0f5c35cf26')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec4c416b-f061-4960-b9f2-8c0f5c35cf26 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def breast_cancer_classifier(df, n_trees):\n",
        "  # 3. I will normalize with the StandardScaler class.\n",
        "\n",
        "  # X contains all columns except the last one, which is the label\n",
        "  X = df.iloc[:, :-1]\n",
        "\n",
        "  # Y contains only the last column\n",
        "  Y = df.iloc[:, -1]\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X) # where X is the dataframe containing the features\n",
        "\n",
        "  # 4. Next, I will perform PCA with number of retained components = 50\n",
        "  pca = PCA(n_components=50)\n",
        "  X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "  # 5. After that, I will use the RF classifier (the sklearn one, not the one I implemented above)\n",
        "  rf = RandomForestClassifier(n_estimators=n_trees,  random_state=42) # n_estimators is the number of trees\n",
        "\n",
        "  # 6. Then, I will split the data into 80% training and 20% testing and check the F1 score based on the rf classifier\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2)\n",
        "  rf.fit(X_train, y_train)\n",
        "  predictions = rf.predict(X_test)\n",
        "  f1 = f1_score(y_test, predictions, average='weighted')  # f1_score takes as input the true labels and the predictions, with this order\n",
        "  # average = 'weighted' because the model aims to predict more than two classes (the subtypes of breast cancer, which are 4 or 5)\n",
        "  # also weighted takes into consideration the frequency of each label in the dataset and it is very good for imbalanced datasets\n",
        "  # if the dataset is not imbalanced, I could use f1_macro or f1_micro, but this is a lot of details for the scope of assignment\n",
        "\n",
        "  # 7. Afterewards, I will do K fold cross validations and compare the mean F1 score with the F1 score from above\n",
        "  f1_scores = cross_val_score(rf, X_pca, Y, cv=5, scoring='f1_weighted') # I use the scoring method to be the f1_weighted because this is what I used in the steps before\n",
        "  # So to make the comparison fair and logical, I have to get the cross validation scores with the same scoring criteria as above\n",
        "  # And that is why I used 'f1_weighted' instead of 'accuracy' for example\n",
        "  avg_f1_score = np.mean(f1_scores) # then I get the mean so that I compare a number with a number\n",
        "\n",
        "  print('The score without cross validations for', n_trees, ' trees is = ', f1)\n",
        "  print('The mean score WITH cross validation for', n_trees, ' trees is = ', avg_f1_score)"
      ],
      "metadata": {
        "id": "MlY1SLz7rqcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6- Discuss the results you got from 4 and 5. (2 mark)\n",
        "\n",
        "breast_cancer_classifier(final_df, 20)\n",
        "\n",
        "# The score without cross validations for 20  trees is =  0.7074821047656361\n",
        "# The mean score WITH cross validation for 20  trees is =  0.8443145469331427\n",
        "\n",
        "# Therefore, the accuracy increases by doing cross validation, which is expected!!\n",
        "\n",
        "# 7- Try different number of trees[50,60,70,80,90,100] in the random forest algorithm and decide which one is better(2 mark)\n",
        "\n",
        "# To do this question, I will simply call the function 6 times, each time with a value from 50 to 100, jumping by 10.\n",
        "for i in range(50,110,10):\n",
        "  breast_cancer_classifier(final_df, i)\n",
        "\n",
        "# The score without cross validations for 50  trees is =  0.8040585334303502\n",
        "# The mean score WITH cross validation for 50  trees is =  0.8875007658209496\n",
        "# The score without cross validations for 60  trees is =  0.904032258064516\n",
        "# The mean score WITH cross validation for 60  trees is =  0.9065389445934737\n",
        "# The score without cross validations for 70  trees is =  0.8947882903740291\n",
        "# The mean score WITH cross validation for 70  trees is =  0.9099635781509345\n",
        "# The score without cross validations for 80  trees is =  0.7612782747507605\n",
        "# The mean score WITH cross validation for 80  trees is =  0.8890015210218947\n",
        "# The score without cross validations for 90  trees is =  0.9025089605734766\n",
        "# The mean score WITH cross validation for 90  trees is =  0.9102690225650377\n",
        "# The score without cross validations for 100  trees is =  0.9368392518297641\n",
        "# The mean score WITH cross validation for 100  trees is =  0.880960266804669\n",
        "\n",
        "# In general, WITH cross validation produces higher accuracy than without cross validation\n",
        "# But for the number of trees, I think it's a little bit random (fluctuates) and has no specific pattern\n",
        "# However, by doing cross validation, I think the score is more stable (less fluctuating) than without doing the cross validation as the number of trees change\n",
        "\n",
        "# By doing a prelimenary run, the optimal combination is [100 trees WITHOUT cross validation], but this is the only time when the WITHOUT cross validation scored higher than WITH cross validation\n",
        "# For all other lesser number of trees, cross validation inclusion increased the score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZEZFAS2pn0c",
        "outputId": "930f170e-ffc8-4c3d-e58d-557874a6948f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score without cross validations for 20  trees is =  0.7074821047656361\n",
            "The mean score WITH cross validation for 20  trees is =  0.8443145469331427\n",
            "The score without cross validations for 50  trees is =  0.8040585334303502\n",
            "The mean score WITH cross validation for 50  trees is =  0.8875007658209496\n",
            "The score without cross validations for 60  trees is =  0.904032258064516\n",
            "The mean score WITH cross validation for 60  trees is =  0.9065389445934737\n",
            "The score without cross validations for 70  trees is =  0.8947882903740291\n",
            "The mean score WITH cross validation for 70  trees is =  0.9099635781509345\n",
            "The score without cross validations for 80  trees is =  0.7612782747507605\n",
            "The mean score WITH cross validation for 80  trees is =  0.8890015210218947\n",
            "The score without cross validations for 90  trees is =  0.9025089605734766\n",
            "The mean score WITH cross validation for 90  trees is =  0.9102690225650377\n",
            "The score without cross validations for 100  trees is =  0.9368392518297641\n",
            "The mean score WITH cross validation for 100  trees is =  0.880960266804669\n"
          ]
        }
      ]
    }
  ]
}